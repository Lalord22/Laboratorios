{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJweR5ztKRdmMybQF4kFE9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Problema:** Clasificación de flores Iris\n"],"metadata":{"id":"SwJg3lV1BnXQ"}},{"cell_type":"markdown","source":["**Descripción del problema:** El conjunto de datos Iris es un conjunto de datos popular en la ciencia de datos y se utiliza para clasificar tres especies diferentes de flores Iris (Setosa, Versicolor y Virginica) en función de cuatro características: longitud del sépalo, ancho del sépalo, longitud del pétalo y ancho del pétalo. El objetivo es construir un modelo de clasificación en Python que pueda predecir la especie de flor Iris en función de estas características."],"metadata":{"id":"Ext9xV7RChi1"}},{"cell_type":"markdown","source":["- **Josafat Argudas Gutiérrez**\n","- **Francisco Arias Sanabria**\n","- **Gerardo Salazar Vargas**\n","\n","**Precisión (Precision):** La precisión es una métrica que mide la proporción de verdaderos positivos (TP) en relación con todos los positivos predichos. En otras palabras, cuántos de los casos clasificados como positivos fueron realmente positivos. La fórmula es la siguiente:\n","\n","$$\n"," \\text{Precisión} = \\frac{TP}{TP + FP} \\\n","$$\n","- TP (Verdaderos positivos): Número de casos positivos correctamente clasificados.\n","- FP (Falsos positivos): Número de casos negativos incorrectamente clasificados como positivos.\n","\n","**Recuperación (Recall):** La recuperación, también conocida como sensibilidad o tasa verdadera positiva, mide la proporción de verdaderos positivos en relación con todos los casos reales positivos. En otras palabras, cuántos de los casos positivos reales fueron detectados correctamente. La fórmula es la siguiente:\n","$$\n"," \\text{Recall} = \\frac{TP}{TP + FN}\n","$$\n","- FN (Falsos negativos): Número de casos positivos reales incorrectamente clasificados como negativos.\n","\n","**Puntuación F1 (F1-score):** La puntuación F1 es una métrica que combina la precisión y la recuperación en un solo número. Es útil cuando se desea encontrar un equilibrio entre ambas métricas. Se calcula mediante la siguiente fórmula:\n","$$\n"," \\text{F1-score} = \\frac{2 \\cdot (\\text{Precisión} \\cdot \\text{Recall})}{\\text{Precisión} + \\text{Recall}}\n","$$\n","La puntuación F1 es especialmente útil cuando los falsos negativos y falsos positivos tienen un impacto significativo en el problema.\n","\n","**Soporte (Support):** El soporte es la cantidad real de casos de una clase particular en el conjunto de datos. Proporciona información sobre cuántos ejemplos pertenecen a cada clase y se utiliza para calcular las métricas anteriores. El soporte se encuentra en la última columna del informe de clasificación.\n","\n","**Precisión global (Accuracy):** La precisión global mide la proporción de predicciones correctas en relación con todas las predicciones realizadas. Es una métrica que evalúa la precisión del modelo en general. La fórmula es la siguiente:\n","$$\n"," \\text{Precisión global (Accuracy)} = \\frac{TP + TN}{TP + TN + FP + FN}\n","$$\n","- TP (Verdaderos positivos): Número de casos positivos correctamente clasificados.\n","- TN (Verdaderos negativos): Número de casos negativos correctamente clasificados.\n","- FP (Falsos positivos): Número de casos negativos incorrectamente clasificados como positivos.\n","- FN (Falsos negativos): Número de casos positivos incorrectamente clasificados como negativos.\n","\n","**Matriz de Confusión (Confusion Matrix):** La matriz de confusión es una tabla que muestra el rendimiento del modelo para diferentes clases. En una matriz de confusión, las filas representan las clases reales y las columnas representan las clases predichas. Las celdas de la matriz indican el número de ejemplos que fueron clasificados en cada categoría. La matriz de confusión se utiliza para calcular todas las métricas anteriores.\n","\n","En resumen, estas métricas son fundamentales en la evaluación de modelos de clasificación y ayudan a entender cómo un modelo se comporta en términos de clasificación de diferentes clases. Cada métrica tiene su propio enfoque y se utiliza en diferentes contextos según las necesidades del problema.\n"],"metadata":{"id":"wyOK4f-BIJEO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvFsmnfIBlAw","executionInfo":{"status":"ok","timestamp":1698953100166,"user_tz":360,"elapsed":2990,"user":{"displayName":"Josafat A.G","userId":"07447741556543188870"}},"outputId":"83fd0b72-5c05-402e-d6ac-55a6491173e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Precisión del modelo: 1.0\n","Matriz de confusión:\n","[[10  0  0]\n"," [ 0  9  0]\n"," [ 0  0 11]]\n","Informe de clasificación:\n","                 precision    recall  f1-score   support\n","\n","    Iris-setosa       1.00      1.00      1.00        10\n","Iris-versicolor       1.00      1.00      1.00         9\n"," Iris-virginica       1.00      1.00      1.00        11\n","\n","       accuracy                           1.00        30\n","      macro avg       1.00      1.00      1.00        30\n","   weighted avg       1.00      1.00      1.00        30\n","\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","def clasificar_iris(url, test_size=0.2, n_neighbors=3):\n","    # Cargar el conjunto de datos Iris desde una URL\n","    column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n","    iris_data = pd.read_csv(url, names=column_names)\n","\n","    # Dividir los datos en características (X) y etiquetas (y)\n","    X = iris_data.iloc[:, :-1].values\n","    y = iris_data['species'].values\n","\n","    # Dividir los datos en conjuntos de entrenamiento y prueba\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n","\n","    # Normalizar las características para que tengan una escala común\n","    scaler = StandardScaler()\n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.transform(X_test)\n","\n","    # Crear un modelo de clasificación de vecinos más cercanos (K-NN)\n","    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n","    knn.fit(X_train, y_train)\n","\n","    # Realizar predicciones en el conjunto de prueba\n","    y_pred = knn.predict(X_test)\n","\n","    # Evaluar el rendimiento del modelo\n","    accuracy = accuracy_score(y_test, y_pred)\n","    confusion = confusion_matrix(y_test, y_pred)\n","    report = classification_report(y_test, y_pred)\n","\n","    return accuracy, confusion, report\n","\n","# Ejemplo de uso\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n","accuracy, confusion, classification_report = clasificar_iris(url)\n","print(f\"Precisión del modelo: {accuracy}\")\n","print(\"Matriz de confusión:\")\n","print(confusion)\n","print(\"Informe de clasificación:\")\n","print(classification_report)"]}]}